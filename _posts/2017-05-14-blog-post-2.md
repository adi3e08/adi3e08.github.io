---
title: 'Random Walk'
date: 2017-05-14
permalink: /posts/2017/05/random-walk/
tags:
  - Tabular RL
  - TD Learning
---

Consider *N* states laid out in a row, with the left most state being a
terminal state. Consider an agent who, when at a non-terminal state,
chooses to move with probability *p* to the left neighbour and with
probability 1â€…âˆ’â€…*p* to the right neighbour(or to itself in case of the
right-most state). All rewards are 1.  
  
**MDP definition**,

-   **State Space** :  
      
    Set of all states, including terminal state,
    ğ’®<sup>+</sup>â€„=â€„{1,â€†2...,â€†*N*}.  
      
    Set of all states, excluding terminal state, ğ’®â€„=â€„{2...,â€†*N*}.

-   **Action Space** :  
      
    For all *s*â€„âˆˆâ€„*S*, set of valid actions at *s*,
    $$\\mathcal{A}(s) = \\begin{cases} 
    \\{-1,0\\} \\ if \\ s = N \\\\
    \\{-1,1\\} \\ if \\ s \\in S-\\{N\\}
    \\end{cases}$$

-   **Reward Structure** :  
      
    For all *s*â€„âˆˆâ€„ğ’®,Â *a*â€„âˆˆâ€„ğ’œ(*s*),
    â„›(*s*,*a*)â€„=â€„ğ”¼\[*R*<sub>*t*â€…+â€…1</sub>\|*S*<sub>*t*</sub>=*s*,*A*<sub>*t*</sub>=*a*\]â€„=â€„1

-   **Transition Model** :  
      
    For all *s*â€„âˆˆâ€„ğ’®,Â *a*â€„âˆˆâ€„ğ’œ(*s*),â€†*s*â€²â€„âˆˆâ€„ğ’®<sup>+</sup>,
    $$\\mathcal{P}(s,a,s')= \\mathbb{P}\[S\_{t+1}=s'\|S\_{t}=s,A\_{t}=a \] = \\begin{cases} 1 \\ if \\ s'= s+a \\\\ 
    0 \\ if \\ s' \\in S^{+}-\\{s+a\\} \\end{cases}$$

**Agentâ€™s Policy**:  
  
For all *s*â€„âˆˆâ€„ğ’®,Â *a*â€„âˆˆâ€„ğ’œ(*s*),
$$\\pi\_{agent}(s,a) = \\mathbb{P}\[A\_{t}=a \|S\_{t}=s\] = 
\\begin{cases} 
if \\ s = N \\
\\begin{cases}
p \\ if \\ a = -1 \\\\
1-p \\ if \\ a = 0 
\\end{cases}
\\\\
if \\ s \\in \\mathcal{S}-\\{N\\} \\
\\begin{cases}
p \\ if \\ a = -1 \\\\
1-p \\ if \\ a = 1 
\\end{cases}
 \\end{cases}$$
**Objective** : Evaluate *v*<sub>*Ï€*<sub>*a**g**e**n**t*</sub></sub>.
Assume *N*â€„=â€„11,Â *p*â€„=â€„0.7,Â *Î³*â€„=â€„1.  
  
In the solutions to follow, we store the Value function *V* in a table
since the State and Action Spaces are discrete and finite.
