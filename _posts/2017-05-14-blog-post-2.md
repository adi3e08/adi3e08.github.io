---
title: 'Random Walk'
date: 2017-05-14
permalink: /posts/2017/05/random-walk/
tags:
  - Tabular RL
  - TD Learning
---

Consider *N* states laid out in a row, with the left most state being a
terminal state. Consider an agent who, when at a non-terminal state,
chooses to move with probability *p* to the left neighbour and with
probability 1 − *p* to the right neighbour(or to itself in case of the
right-most state). All rewards are 1.  
  
**MDP definition**,

-   **State Space** :  
      
    Set of all states, including terminal state,
    𝒮<sup>+</sup> = {1, 2..., *N*}.  
      
    Set of all states, excluding terminal state, 𝒮 = {2..., *N*}.

-   **Action Space** :  
      
    For all *s* ∈ *S*, set of valid actions at *s*,
    $$\\mathcal{A}(s) = \\begin{cases} 
    \\{-1,0\\} \\ if \\ s = N \\\\
    \\{-1,1\\} \\ if \\ s \\in S-\\{N\\}
    \\end{cases}$$

-   **Reward Structure** :  
      
    For all *s* ∈ 𝒮, *a* ∈ 𝒜(*s*),
    ℛ(*s*,*a*) = 𝔼\[*R*<sub>*t* + 1</sub>\|*S*<sub>*t*</sub>=*s*,*A*<sub>*t*</sub>=*a*\] = 1

-   **Transition Model** :  
      
    For all *s* ∈ 𝒮, *a* ∈ 𝒜(*s*), *s*′ ∈ 𝒮<sup>+</sup>,
    $$\\mathcal{P}(s,a,s')= \\mathbb{P}\[S\_{t+1}=s'\|S\_{t}=s,A\_{t}=a \] = \\begin{cases} 1 \\ if \\ s'= s+a \\\\ 
    0 \\ if \\ s' \\in S^{+}-\\{s+a\\} \\end{cases}$$

**Agent’s Policy**:  
  
For all *s* ∈ 𝒮, *a* ∈ 𝒜(*s*),
$$\\pi\_{agent}(s,a) = \\mathbb{P}\[A\_{t}=a \|S\_{t}=s\] = 
\\begin{cases} 
if \\ s = N \\
\\begin{cases}
p \\ if \\ a = -1 \\\\
1-p \\ if \\ a = 0 
\\end{cases}
\\\\
if \\ s \\in \\mathcal{S}-\\{N\\} \\
\\begin{cases}
p \\ if \\ a = -1 \\\\
1-p \\ if \\ a = 1 
\\end{cases}
 \\end{cases}$$
**Objective** : Evaluate *v*<sub>*π*<sub>*a**g**e**n**t*</sub></sub>.
Assume *N* = 11, *p* = 0.7, *γ* = 1.  
  
In the solutions to follow, we store the Value function *V* in a table
since the State and Action Spaces are discrete and finite.
