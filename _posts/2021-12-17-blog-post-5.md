---
title: 'Notes on Soft Actor-Critic'
date: 2021-12-17
permalink: /posts/2021/12/soft-actor-critic/
tags:
  - Model Free RL
  - Maximum Entropy RL
---
Soft Actor-Critic (SAC) [[1]](#1) [[2]](#2) is a state-of-the-art model-free deep RL algorithm for continuous action spaces. It adopts an off-policy actor-critic approach and uses stochastic policies. SAC uses the maximum entropy formulation to achieve exploration.

## Maximum Entropy RL
In maximum entropy RL, the objective is to maximize the expected return while acting as randomly as possible. By doing so, the agent can capture different modes of optimality. This also improves robustness against environmental changes.
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/max_ent_rl_3.gif" width="50%" height="50%"/>
<br>
<br>
A robot navigating a maze. A policy trained using the maximum entropy RL objective will explore both passages during training.
</p>
The entropy of a random variable is defined as, $H(X) = \underset{x \sim P}{\mathbb{E}}[-\log P(x)] $.

The maximum entropy RL objective is given by,
\\[ \pi^{\*} = \underset{\pi}{\arg\max} \underset{\tau \sim \pi}{\mathbb{E}} \bigg[\sum_{t=0}^{\infty}\gamma^{t}\bigg(\;r(s_{t},a_{t},s_{t+1})+\alpha H(\pi(\cdot|s_{t}))\;\bigg)\bigg] \\]

Here $\alpha > 0$ , is the weightage given to the entropy term in the objective. $\alpha$ is also referred to as "temperature".

We define the value function to include the entropy bonuses from every timestep,
\\[ V^\pi(s) = \underset{\tau \sim \pi}{\mathbb{E}}\bigg[\sum_{t=0}^{\infty}\gamma^{t}\bigg(\;r(s_{t},a_{t},s_{t+1})+\alpha H(\pi(\cdot|s_{t}))\;\bigg)\;\bigg|\;s_{0}=s\,\bigg] \\]

We define the action-value function to include the entropy bonuses from every timestep except the first,
\\[ Q^\pi(s,a) = \underset{\tau \sim \pi}{\mathbb{E}}\bigg[\sum_{t=0}^{\infty}\gamma^{t} r(s_{t},a_{t},s_{t+1}) + \alpha \sum_{t=1}^{\infty} \gamma^{t} H(\pi(\cdot|s_{t})) \;\bigg|\;s_{0}=s,a_{0}=a\,\bigg] \\]

Thus,
\\[ V^\pi(s) = \underset{a \sim \pi}{\mathbb{E}}[Q^\pi(s,a)] + \alpha H(\pi(\cdot|s)) \\]

## Soft Actor-Critic (SAC)
In SAC we have,
- a single policy network, $ \pi_{\theta} $
- two Q networks $Q_{w_{1}} \; , \; Q_{w_{2}}$
- two target Q networks $Q_{w_{1}^{'}} \; , \; Q_{w_{2}^{'}}$

### Learning the Q functions
Both Q-functions are learned with Mean Squared Bellman Error minimization, by regressing to a single shared target y.
\\[L(w_{i}) = \underset{(s,a,r,s')\sim \mathcal{D}}{\mathbb{E}}[\;( Q_{w_{i}}(s,a)-y )^{2}\;]\\]

The shared target y is computed using target Q-networks and makes use of the clipped double-Q trick.
\\[y = r + \gamma \; (\; \underset{i=1,2}{\min} Q_{w_{i}^{'}}(s',a') - \alpha \log \pi_{\theta}(a'|s') \;)\\]

The next-state actions used in the target come from the current policy instead of the target policy.

### Learning the policy
The objective is to maximize
\\[ V^\pi(s) = \underset{a \sim \pi}{\mathbb{E}}[Q^\pi(s,a)] + \alpha H(\pi(\cdot|s)) \\]

The policy is stochastic, therefore actions are sampled. To be able to backprop through sampled actions, we use the reparameterization trick, 
\\[ a = a_{\theta}(s,\epsilon) = \text{tanh}(\mu_{\theta}(s)+\sigma_{\theta}(s)\cdot \epsilon) \\]

The policy outputs mean $\mu$ and standard deviation $\sigma$ of a Gaussian distribution. We then sample a gaussian noise $\epsilon \sim \mathcal{N}(0,\mathbb{I})$. We combine the noise with the policy outputs and use tanh to squash the action to [-1,1].

Thus we can rewrite the expectation over actions into an expectation over noise,
\\[ \underset{a\sim \pi_{\theta}}{\mathbb{E}}[\; Q^{\pi_{\theta}}(s,a) - \alpha \log \pi_{\theta}(a|s) \;] = \underset{\epsilon \sim\mathcal{N}}{\mathbb{E}}[\; Q^{\pi_{\theta}}(s,a_{\theta}(s,\epsilon)) - \alpha \log \pi_{\theta}(a_{\theta}(s,\epsilon)|s) \;] \\]

Thus the final objective becomes
\\[ \underset{\theta}{\max} \underset{\epsilon \sim\mathcal{N}}{\underset{s\sim \mathcal{D}}{\mathbb{E}}} [\; (\; \underset{i=1,2}{\min} Q_{w_{i}}(s,a_{\theta}(s,\epsilon)) - \alpha \log \pi_{\theta}(a_{\theta}(s,\epsilon)|s) \;] \\]

### Algorithm
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/sac_algo.png" width="100%" height="100%"/>
</p>

### Automatic temperature tuning
In SAC v1, the temperature $\alpha$ is a hyperparameter. However it was found that the algorithm is brittle to the choice of $\alpha$. 

In SAC v2, the temperature $\alpha$ is learnt by minimizing the loss,
\\[ L(\alpha) = \alpha \; (-\log\pi(a|s)-\widetilde{H}) \\] 
where $\widetilde{H}$ is the entropy target. Typically, $\widetilde{H}$ is set to be equal to the negative of the action space dimension i. e. $\widetilde{H} = - \; \text{dim}(\mathcal{A})$.

## Pytorch Implementation
You can find my Pytorch implementation of SAC for continuous action spaces [here](https://github.com/adi3e08/SAC){:target="_blank"}. 

### Results
I trained SAC on a few continuous control tasks from [Deepmind Control Suite](https://github.com/deepmind/dm_control/tree/master/dm_control/suite) and [OpenAI Gym](https://www.gymlibrary.ml/). The results are below,

* Cartpole Swingup ([Deepmind Control Suite](https://github.com/deepmind/dm_control/tree/master/dm_control/suite)) - Swing up and balance an unactuated pole by applying forces to a cart at its base.
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/sac_cartpole_swingup.png" width="50%" height="50%"/>
</p>
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/sac_cartpole_swingup.gif" width="50%" height="50%"/>
</p>

* Reacher Hard ([Deepmind Control Suite](https://github.com/deepmind/dm_control/tree/master/dm_control/suite)) - Control a two-link robotic arm to reach a randomized target location.
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/sac_reacher_hard.png" width="50%" height="50%"/>
</p>
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/sac_reacher_hard.gif" width="50%" height="50%"/>
</p>

* [Bipedal Walker](https://www.gymlibrary.ml/environments/box2d/bipedal_walker/) (OpenAI Gym) - Train a bipedal robot to walk.
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/sac_bipedal_walker.png" width="50%" height="50%"/>
</p>
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/sac_bipedal_walker.gif" width="50%" height="50%"/>
</p>

### References
<a id="1">[1]</a>
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine.
Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. 2018.
[Link](https://arxiv.org/abs/1801.01290)

<a id="1">[1]</a>
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, Sergey Levine.
Soft Actor-Critic: Algorithms and Applications. 2018.
[Link](https://arxiv.org/abs/1812.05905)
