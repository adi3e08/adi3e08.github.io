---
title: 'Soft Actor Critic explained'
date: 2022-05-17
permalink: /posts/2022/05/soft-actor-critic/
tags:
  - Model Free RL
  - Maximum Entropy RL
---
- Stochastic, off-policy, model-free RL algorithm
- Uses maximum entropy formulation to encourage stability and exploration
- Sample-efficient
- Scales to high-dimensional observation/action spaces
- Robust to random seeds, noise etc.
- State of the art!

v1 : "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", Haarnoja et al

v2 : "Soft Actor-Critic: Algorithms and Applications", Haarnoja et al

## Maximum Entropy RL
- Maximize expected return while acting as randomly as possible
- Agent can capture different modes of optimality to improve robustness against environmental changes
- Entropy of a random variable
  \[ H(P) = \underset{x \sim P}{\mathbb{E}}[-\log P(x)] \]
- Maximum entropy RL objective
\[ \pi^{*} = \underset{\pi}{\arg\max} \underset{\tau \sim \pi}{\mathbb{E}}\bigg[\sum_{t=0}^{\infty}\gamma^{t}\bigg(\;r(s_{t},a_{t},s_{t+1})+\alpha H(\pi(\cdot|s_{t}))\;\bigg)\bigg] \]
- Here $$\alpha > 0$$ , is the weightage given to the entropy term in the objective. $$\alpha$$ is sometimes referred to as "temperature"
- Define the value function to include the entropy bonuses from every timestep
\[ V^\pi(s) = \underset{\tau \sim \pi}{\mathbb{E}}\bigg[\sum_{t=0}^{\infty}\gamma^{t}\bigg(\;r(s_{t},a_{t},s_{t+1})+\alpha H(\pi(\cdot|s_{t}))\;\bigg)\;\bigg|\;s_{0}=s\,\bigg] \]
- Define the action-value function to include the entropy bonuses from every timestep except the first
\[ Q^\pi(s,a) = \underset{\tau \sim \pi}{\mathbb{E}}\bigg[\sum_{t=0}^{\infty}\gamma^{t} r(s_{t},a_{t},s_{t+1}) + \alpha \sum_{t=1}^{\infty} \gamma^{t} H(\pi(\cdot|s_{t})) \;\bigg|\;s_{0}=s,a_{0}=a\,\bigg] \]
- Thus
\[ V^\pi(s) = \underset{a \sim \pi}{\mathbb{E}}[Q^\pi(s,a)] + \alpha H(\pi(\cdot|s)) \]
