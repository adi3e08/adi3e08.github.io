---
title: 'Soft Actor Critic simplified'
date: 2022-05-17
permalink: /posts/2022/05/soft-actor-critic/
tags:
  - Model Free RL
  - Maximum Entropy RL
---
Soft Actor-Critic (SAC) is a state-of-the-art model-free RL algorithm for continuous action spaces. It uses stochastic policies and adopts an off-policy approach. SAC uses the maximum entropy formulation to encourage stability and exploration. It is sample-efficient, scales to high-dimensional observation/action spaces and robust to random seeds, noise etc.
### Maximum Entropy RL
In maximum entropy RL, the objective is to maximize the expected return while acting as randomly as possible. By doing so, the agent can capture different modes of optimality. This improves robustness against environmental changes.
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/max_ent_rl_1.png" width="50%" height="50%"/>
<br>
A robot navigating a maze.
</p>
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/max_ent_rl_2.png" width="50%" height="50%"/>
<br>
A multimodal Q function.
</p>
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/max_ent_rl_3.gif" width="50%" height="50%"/>
<br>
A policy trained using the maximum entropy RL objective will explore both passages during training.
</p>
- Entropy of a random variable
  $$ H(X) = \underset{x \sim P}{\mathbb{E}}[-\log P(x)] $$
- Maximum entropy RL objective
$$ \pi^{*} = \underset{\pi}{\arg\max} \underset{\tau \sim \pi}{\mathbb{E}}\bigg[\sum_{t=0}^{\infty}\gamma^{t}\bigg(\;r(s_{t},a_{t},s_{t+1})+\alpha H(\pi(\cdot|s_{t}))\;\bigg)\bigg] $$
- Here $$\alpha > 0$$ , is the weightage given to the entropy term in the objective. $$\alpha$$ is sometimes referred to as "temperature"
- Define the value function to include the entropy bonuses from every timestep
$$ V^\pi(s) = \underset{\tau \sim \pi}{\mathbb{E}}\bigg[\sum_{t=0}^{\infty}\gamma^{t}\bigg(\;r(s_{t},a_{t},s_{t+1})+\alpha H(\pi(\cdot|s_{t}))\;\bigg)\;\bigg|\;s_{0}=s\,\bigg] $$
- Define the action-value function to include the entropy bonuses from every timestep except the first
$$ Q^\pi(s,a) = \underset{\tau \sim \pi}{\mathbb{E}}\bigg[\sum_{t=0}^{\infty}\gamma^{t} r(s_{t},a_{t},s_{t+1}) + \alpha \sum_{t=1}^{\infty} \gamma^{t} H(\pi(\cdot|s_{t})) \;\bigg|\;s_{0}=s,a_{0}=a\,\bigg] $$
- Thus
$$ V^\pi(s) = \underset{a \sim \pi}{\mathbb{E}}[Q^\pi(s,a)] + \alpha H(\pi(\cdot|s)) $$

### SAC
- Policy, $$ \pi_{\theta} $$
- Two Q functions $$Q_{w_{1}} \; , \; Q_{w_{2}}$$
- Two target Q functions $$Q_{w_{1}^{'}} \; , \; Q_{w_{2}^{'}}$$
- SAC v1 : Temperature $$\alpha$$ is a hyperparameter
- SAC v2 : Temperature $$\alpha$$ is learnt

### Learning the Q functions
- Both Q-functions are learned with Mean Squared Bellman Error minimization, by regressing to a single shared target y.
$$L(w_{i}) = \underset{(s,a,r,s')\sim \mathcal{D}}{\mathbb{E}}[\;( Q_{w_{i}}(s,a)-y )^{2}\;]$$
- The shared target y is computed using target Q-networks and makes use of the clipped double-Q trick.
$$y = r + \gamma \; (\; \underset{i=1,2}{\min} Q_{w_{i}^{'}}(s',a') - \alpha \log \pi_{\theta}(a'|s') \;)$$
- The next-state actions used in the target come from the current policy instead of the target policy.
$$a' \sim \pi(\cdot|s')$$

### Learning the policy
-  Maximize
$$ V^\pi(s) = \underset{a \sim \pi}{\mathbb{E}}[Q^\pi(s,a)] + \alpha H(\pi(\cdot|s)) $$
- Policy is stochastic, therefore actions are sampled.
- To be able to backprop through sampled actions, we use the reparameterization trick
- Policy outputs mean $$\mu$$ and standard deviation $$\sigma$$ of a Gaussian distribution
- We then sample a gaussian noise $$\epsilon \sim \mathcal{N}(0,\mathbb{I})$$
- We combine the noise with the policy outputs and Use tanh to squash the action to [-1,1]
$$ a = a_{\theta}(s,\epsilon) = \text{tanh}(\mu_{\theta}(s)+\sigma_{\theta}(s)\cdot \epsilon) $$
- Thus we can rewrite the expectation over actions into an expectation over noise,
$$ \underset{a\sim \pi_{\theta}}{\mathbb{E}}[\; Q^{\pi_{\theta}}(s,a) - \alpha \log \pi_{\theta}(a|s) \;] = \underset{\epsilon \sim\mathcal{N}}{\mathbb{E}}[\; Q^{\pi_{\theta}}(s,a_{\theta}(s,\epsilon)) - \alpha \log \pi_{\theta}(a_{\theta}(s,\epsilon)|s) \;] $$
- Thus the final objective becomes
$$ \underset{\theta}{\max} \underset{\epsilon \sim\mathcal{N}}{\underset{s\sim \mathcal{D}}{\mathbb{E}}} [\; (\; \underset{i=1,2}{\min} Q_{w_{i}}(s,a_{\theta}(s,\epsilon)) - \alpha \log \pi_{\theta}(a_{\theta}(s,\epsilon)|s) \;] $$

### SAC algorithm
<p align="center">
<img src="https://adi3e08.github.io/files/blog/soft-actor-critic/imgs/sac_algo.png" width="75%" height="75%"/>
</p>

### References
- "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", Haarnoja et al. [Link](https://arxiv.org/abs/1801.01290)
- "Soft Actor-Critic: Algorithms and Applications", Haarnoja et al. [Link](https://arxiv.org/abs/1812.05905)
