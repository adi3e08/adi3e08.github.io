---
title: 'Soft Actor Critic explained'
date: 2022-05-17
permalink: /posts/2022/05/soft-actor-critic/
tags:
  - Model Free RL
  - Maximum Entropy RL
---
- Stochastic, off-policy, model-free RL algorithm
- Uses maximum entropy formulation to encourage stability and exploration
- Sample-efficient
- Scales to high-dimensional observation/action spaces
- Robust to random seeds, noise etc.
- State of the art!

v1 : "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", Haarnoja et al

v2 : "Soft Actor-Critic: Algorithms and Applications", Haarnoja et al

## Maximum Entropy RL
- Maximize expected return while acting as randomly as possible
- Agent can capture different modes of optimality to improve robustness against environmental changes
- Entropy of a random variable
  $$ H(P) = \underset{x \sim P}{\mathbb{E}}[-\log P(x)] $$
- Maximum entropy RL objective
