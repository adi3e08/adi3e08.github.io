---
title: 'Notes on DDPG'
date: 2022-05-17
permalink: /posts/2022/05/ddpg/
tags:
  - DDPG
---
Deep Deterministic Policy Gradient (DDPG) [[1]](#1), is a model-free deep RL algorithm for continuous action spaces. It adopts an off-policy actor-critic
approach and uses deterministic policies.

DDPG mainly draws from two previous papers, Deep Q Networks (DQN) [[2]](#2) and Deterministic Policy Gradients (DPG) [[3]](#3).

DQN combined deep learning and reinforcement learning successfully for the first time and trained an agent to play atari video games using just pixels as input. DQN is based on the Q-learning algorithm [[4]](#4), a popular off-policy algortihm which learns a greedy policy while following a exploratory behaviour policy (e.g $\epsilon$-greedy).
\\[ \delta_{t} = r_{t} + \gamma \max_{a'} Q^{w}(s_{t+1},a') - Q^{w}(s_{t},a_{t}) \\]
\\[ w_{t+1} = w_{t} + \alpha_{w} \delta_{t} \nabla_{w} Q^{w}(s_{t},a_{t}) \\]

DQN adapts Q-learning to use deep neural networks. To achieve stable learning while using large, non-linear function approximators, DQN uses two main innovations, 
- Experience replay : the network is trained off-policy with samples from a replay buffer to minimize correlations between samples.
- Target networks : use target Q-network to give consistent targets during temporal difference backups.

It is not possible to directly apply DQN to problems with continuous action spaces, as the Q-learning update requires computing a maximum over the action space, which in the case of continuous actions would require an iterative optimization process for every update.

DDPG, instead adopts an actor-critic approach based on the DPG algorithm. DPG learns a deterministic target policy $\mu$, while following a stochastic behaviour policy $\beta$, using the off policy deterministic policy gradient. It learns a critic through a Q-learning update.
\\[ \nabla_{\theta} J(\theta) \approx \mathbb{E}_{s \sim \rho^{\beta}}[\nabla_{\theta} \mu_{_{\theta}}(s)\nabla_{a} Q^{w}(s,a)|_{a=\mu_{_{\theta}}(s)}] \\]
\\[ \theta_{t+1} = \theta_{t} + \alpha_{\theta} \nabla_{\theta} \mu_{_{\theta}}(s_{t})\nabla_{a} Q^{w}(s_{t},a)|_{a=\mu_{_{\theta}}(s_{t})} \label{eq-4} \\]
\\[ \delta_{t} = r_{t} + \gamma Q^{w}(s_{t+1},\mu_{_{\theta}}(s_{t+1})) - Q^{w}(s_{t},a_{t}) \label{eq-5} \\]
\\[ w_{t+1} = w_{t} + \alpha_{w} \delta_{t} \nabla_{w} Q^{w}(s_{t},a_{t}) \label{eq-6} \\]

## References
<a id="1">[1]</a> 
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 
Continuous control with deep reinforcement learning. 2015.

<a id="2">[2]</a> 
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller.
Playing atari with deep reinforcement learning. 2013.

<a id="3">[3]</a> 
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. 2014.

<a id="4">[4]</a> 
Christopher J. C. H. Watkins and Peter Dayan. Q-learning. 1992.
