---
title: 'Notes on DDPG'
date: 2022-05-17
permalink: /posts/2022/05/ddpg/
tags:
  - DDPG
---
Deep Deterministic Policy Gradient (DDPG) [[1]](#1), is a model-free deep RL algorithm for continuous action spaces. It adopts an off-policy actor-critic
approach and uses deterministic policies.

DDPG mainly draws from two previous papers, Deep Q Networks (DQN) [[2]](#2) and Deterministic Policy Gradients (DPG) [[3]](#3).

## DQN
DQN combined deep learning and RL successfully for the first time and trained an agent to play atari video games using just pixels as input. DQN is based on the Q-learning algorithm [[4]](#4), a popular off-policy algortihm which learns a greedy policy while following a exploratory behaviour policy (e.g $\epsilon$-greedy). DQN adapts Q-learning to use deep neural networks. To achieve stable learning while using large, non-linear function approximators, DQN uses two main innovations, 
- Experience replay : the network is trained off-policy with samples from a replay buffer to minimize correlations between samples.
- Target networks : use target Q-network to give consistent targets during temporal difference backups.

The loss function used to train the Q-network is given by, 
\\[
L(w) = \underset{(s,a,r,s')\sim D}{\mathbb{E}} [\; ( r + \gamma \max_{a'} Q^{w'}(s',a') - Q^{w}(s,a) )^{2} \;] \nonumber
 \\]
 It's gradient is given by,
\\[
\nabla_{w} L(w) = \underset{(s,a,r,s')\sim D}{\mathbb{E}} [\; - 2 \, ( r + \gamma \max_{a'} Q^{w'}(s',a') - Q^{w}(s,a) ) \nabla_{w} Q^{w}(s,a) \;] \nonumber
 \\]
It is not possible to directly apply DQN to problems with continuous action spaces, as the Q-learning update requires computing a maximum over the action space, which in the case of continuous actions would require an iterative optimization process for every update.

DDPG, instead adopts an actor-critic approach based on the DPG algorithm. 

## DPG
The DPG paper compares different policy gradient algorithms for reinforcement learning on continuous action spaces. DPG showed for the first time that the deterministic policy gradient actually exists and that it is simply the expected gradient of the action value function.

$$\begin{align}
\nabla_{\theta} J(\mu_{_{\theta}}) &= \int_{\mathcal{S}} \rho^{\mu}(s) \nabla_{\theta} \mu_{_{\theta}}(s)\nabla_{a} Q^{\mu}(s,a)|_{a=\mu_{_{\theta}}(s)} ds \nonumber \\
&= \mathbb{E}_{s \sim \rho^{\mu}}[\nabla_{\theta} \mu_{_{\theta}}(s)\nabla_{a} Q^{\mu}(s,a)|_{a=\mu_{_{\theta}}(s)}] \nonumber \\
&= \mathbb{E}_{s \sim \rho^{\mu}}[\nabla_{\theta} Q^{\mu}(s,\mu_{_{\theta}}(s))] \nonumber 
\end{align}$$

Now??
## DDPG
DDPG adapts DPG to use deep neural networks. To ensure stability while training with deep neural networks, DDPG uses the ideas underlying the success of DQN such as experience replay and target networks. To work with observations containing different physical quantities (e.g position, velocity), whose ranges may vary across environments, DDPG uses batch normalization. The overall DDPG algorithm is summarized below.
<p align="center">
<img src="https://adi3e08.github.io/files/blog/ddpg/imgs/ddpg_algo.png" width="100%" height="100%"/>
</p>

## References
<a id="1">[1]</a> 
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 
Continuous control with deep reinforcement learning. 2015.

<a id="2">[2]</a> 
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller.
Playing atari with deep reinforcement learning. 2013.

<a id="3">[3]</a> 
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. 2014.

<a id="4">[4]</a> 
Christopher J. C. H. Watkins and Peter Dayan. Q-learning. 1992.
